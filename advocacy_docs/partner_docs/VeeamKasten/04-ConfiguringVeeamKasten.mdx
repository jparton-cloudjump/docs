---
title: 'Configuration'
description: 'Walkthrough on configuring the integration'
---

Implementing EDB Postgres for Kubernetes with Veeam Kasten requires the following components:

- EDB Postgres for Kubernetes
- EDB Postgres for Kubernetes external backup adapter 
- Veeam Kasten K10 

## Prerequisites

- EDB Postgres for Kubernetes configured and running
- EDB Postgres for Kubernetes external backup adapter configured per your system requirements
- Veeam Kasten K10 installed on your system


## Install the Operator

If you already have the EDB operator installed on Kubernetes you may skip this step.

```bash
kubectl apply -f https://get.enterprisedb.io/cnp/postgresql-operator-1.19.1.yaml
```

Running this command will create the operator namespace where the controller will be running.

## Create an EDB Cluster, Client and Add Data

If, in your environment, you already have an EDB cluster configured you may skip this step.

1. Initiate the below lines of code in your Kubernetes environment.

```bash
kubctl create ns edb
kubectl apply -f cluster-example.yaml -n edb
```
2. Wait until the cluster is completely ready.

3. Install the cnp plugin, if you have already installed this you may skip this.

```bash
curl -sSfL \
  https://github.com/EnterpriseDB/kubectl-cnp/raw/main/install.sh | \
  sudo sh -s -- -b /usr/local/bin
```

4. Create a client certificate to the database.

```bash
kubectl cnp certificate cluster-app \
  --cnp-cluster cluster-example \
  --cnp-user app \
  -n edb
```

**cluster-example** Example:

```bash
# Example of PostgreSQL cluster
apiVersion: postgresql.k8s.enterprisedb.io/v1
kind: Cluster
metadata:
  name: cluster-example
  annotations:
    "k8s.enterprisedb.io/addons": '["external-backup-adapter-cluster"]'
    "k8s.enterprisedb.io/externalBackupAdapterClusterConfig": |-
      electedResourcesDecorators:
        - key: "kasten-enterprisedb.io/elected"
          metadataType: "label"
          value: "true"
      excludedResourcesDecorators:
        - key: "kasten-enterprisedb.io/excluded"
          metadataType: "label"
          value: "true"
        - key: "kasten-enterprisedb.io/excluded-reason"
          metadataType: "annotation"
          value: "Not necessary for backup"
      backupInstanceDecorators:
        - key: "kasten-enterprisedb.io/hasHooks"
          metadataType: "label"
          value: "true"
        - key: "kanister.kasten.io/blueprint"
          metadataType: "annotation"
          value: "edb-hooks"
      preBackupHookConfiguration:
        container:
          key: "kasten-enterprisedb.io/pre-backup-container"
        command:
          key: "kasten-enterprisedb.io/pre-backup-command"
        onError:
          key: "kasten-enterprisedb.io/pre-backup-on-error"
      postBackupHookConfiguration:
        container:
          key: "kasten-enterprisedb.io/post-backup-container"
        command:
          key: "kasten-enterprisedb.io/post-backup-command"
spec:
  instances: 3
  # Example of rolling update strategy:
  # - unsupervised: automated update of the primary once all
  #                 replicas have been upgraded (default)
  # - supervised: requires manual supervision to perform
  #               the switchover of the primary
  primaryUpdateStrategy: unsupervised
  # Require 1Gi of space
  storage:
    size: 1Gi
``` 

5. Create the client.

```bash
kubectl create -f client.yaml -n edb 
```
**client.yaml** Example:
```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cert-test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webtest
  template:
    metadata:
      labels:
        app: webtest
    spec:
      containers:
        - image: ghcr.io/cloudnative-pg/webtest:1.6.0
          name: cert-test
          volumeMounts:
            - name: secret-volume-root-ca
              mountPath: /etc/secrets/ca
            - name: secret-volume-app
              mountPath: /etc/secrets/app
          ports:
            - containerPort: 8080
          env:
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: cluster-example-app
                  key: password
            - name: DATABASE_URL
              value: >
                sslkey=/etc/secrets/app/tls.key
                sslcert=/etc/secrets/app/tls.crt
                sslrootcert=/etc/secrets/ca/ca.crt
                host=cluster-example-rw.default.svc
                dbname=app
                user=app
                sslmode=verify-full
            - name: SQL_QUERY
              value: SELECT 1
      volumes:
        - name: secret-volume-root-ca
          secret:
            secretName: cluster-example-ca
            defaultMode: 0600
        - name: secret-volume-app
          secret:
            secretName: cluster-app
            defaultMode: 0600
```

6. Add some data.

```bash
kubectl exec -it deploy/cert-test -- bash
psql 'sslkey=/etc/secrets/app/tls.key sslcert=/etc/secrets/app/tls.crt sslrootcert=/etc/secrets/ca/ca.crt host=cluster-example-rw dbname=app user=app sslmode=verify-full'
\c app
DROP TABLE IF EXISTS links;
CREATE TABLE links (
	id SERIAL PRIMARY KEY,
	url VARCHAR(255) NOT NULL,
	name VARCHAR(255) NOT NULL,
	description VARCHAR (255),
        last_update DATE
);
INSERT INTO links (url, name, description, last_update) VALUES('https://kasten.io','Kasten','Backup on kubernetes',NOW());
select * from links;
\q
exit
```

## Add the Backup Decorator Annotations to the Cluster

If you created your cluster with the pieces from the previous section the **cluster-example** already includes the backup decorator, so you will not need to do this step again.  If you are working with your own cluster you will need to add the backup decorator.

1.  Add the following annotations to your cluster:

```bash
"k8s.enterprisedb.io/addons": '["external-backup-adapter-cluster"]'
    "k8s.enterprisedb.io/externalBackupAdapterClusterConfig": |-
      electedResourcesDecorators:
        - key: "kasten-enterprisedb.io/elected"
          metadataType: "label"
          value: "true"
      excludedResourcesDecorators:
        - key: "kasten-enterprisedb.io/excluded"
          metadataType: "label"
          value: "true"
        - key: "kasten-enterprisedb.io/excluded-reason"
          metadataType: "annotation"
          value: "Not necessary for backup"
      backupInstanceDecorators:
        - key: "kasten-enterprisedb.io/hasHooks"
          metadataType: "label"
          value: "true"
        - key: "kanister.kasten.io/blueprint"
          metadataType: "annotation"
          value: "edb-hooks"
      preBackupHookConfiguration:
        container:
          key: "kasten-enterprisedb.io/pre-backup-container"
        command:
          key: "kasten-enterprisedb.io/pre-backup-command"
        onError:
          key: "kasten-enterprisedb.io/pre-backup-on-error"
      postBackupHookConfiguration:
        container:
          key: "kasten-enterprisedb.io/post-backup-container"
        command:
          key: "kasten-enterprisedb.io/post-backup-command"
```

## Install the EDB blueprint

1. Enter the follwing command in your environment:

```bash
kubectl create -f edb-hooks.yaml
```

**edb-hooks.yaml** Example:

```bash
apiVersion: cr.kanister.io/v1alpha1
kind: Blueprint
metadata:
  name: edb-hooks
  namespace: kasten-io
actions:
  backupPrehook:
    phases:
    - func: KubeTask
      name: edbPreBackupHook
      args:
        image: ghcr.io/kanisterio/kanister-kubectl-1.18:0.91.0
        command:
          - bash
          - -x 
          - -o
          - errexit
          - -o
          - pipefail
          - -c
          - |
            namespace={{ .Namespace.Name }}
            selector='kasten-enterprisedb.io/hasHooks=true'
            for pod in $(kubectl get po --no-headers -n $namespace -l $selector|awk '{print $1}')
            do
              preCommand=$(kubectl get po -n $namespace $pod -o jsonpath='{.metadata.annotations.kasten-enterprisedb\.io/pre-backup-command}')
              preOnErrorCommand=$(kubectl get po -n $namespace $pod -o jsonpath='{.metadata.annotations.kasten-enterprisedb\.io/pre-backup-on-error}')
              container=$(kubectl get po -n $namespace $pod -o jsonpath='{.metadata.annotations.kasten-enterprisedb\.io/pre-backup-container}')
              command=${preCommand//[\[\]\"\,]/' '} 
              result=$(kubectl exec -it $pod -c $container -n $namespace $pod -- bash -c "if $command; then echo success; else echo failure; fi" | tail -1)
              if [[ $result == "failure" ]]
              then 
                echo "Error after running $preCommand in $pod/$container"
                echo "Executing $preOnErrorCommand"
                command=${preOnErrorCommand//[\[\]\"\,]/' '}
                kubectl exec -it $pod -c $container -n $namespace $pod -- bash -c $command
                exit 1
              fi
            done
            exit 0  
  backupPosthook:
    phases:
    - func: KubeTask
      name: edbPostBackupHook
      args:
        image: ghcr.io/kanisterio/kanister-kubectl-1.18:0.91.0
        command:
          - bash
          - -x 
          - -o
          - errexit
          - -o
          - pipefail
          - -c
          - |
            namespace={{ .Namespace.Name }}
            selector='kasten-enterprisedb.io/hasHooks=true'
            for pod in $(kubectl get po --no-headers -n $namespace -l $selector|awk '{print $1}') 
            do
              postCommand=$(kubectl get po -n $namespace $pod -o jsonpath='{.metadata.annotations.kasten-enterprisedb\.io/post-backup-command}')
              container=$(kubectl get po -n $namespace $pod -o jsonpath='{.metadata.annotations.kasten-enterprisedb\.io/post-backup-container}')
              command=${postCommand//[\[\]\"\,]/' '} 
              result=$(kubectl exec -it $pod -c $container -n $namespace $pod -- bash -c "if $command; then echo success; else echo failure; fi" | tail -1)
              if [[ $result == "failure" ]]
              then 
                echo "Error after running $postCommand in $pod/$container"              
                exit 1
              fi
            done
            exit 0
```





